{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.linalg import block_diag\n",
    "from abc import ABC, abstractmethod\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def _npmle_em_hd(f, Z, mu, covInv, em_iter, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Performs Nonparametric Maximum Likelihood Estimation (NPMLE) using EM.\n",
    "\n",
    "    - Ensures no division by zero in posterior probabilities.\n",
    "    - Ensures `pi` remains well-defined.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : ndarray (n_samples, d)\n",
    "        Observed data points.\n",
    "    Z : ndarray (n_support, d)\n",
    "        Support points.\n",
    "    mu : ndarray (d, d)\n",
    "        Mean transformation matrix.\n",
    "    covInv : ndarray (d, d)\n",
    "        Inverse covariance matrix.\n",
    "    em_iter : int\n",
    "        Number of EM iterations.\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small positive value to prevent division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : ndarray (n_support,)\n",
    "        Estimated prior probabilities.\n",
    "    \"\"\"\n",
    "    nsupp = Z.shape[0]\n",
    "    pi = np.full(nsupp, 1 / nsupp, dtype=np.float64)  # Initialize uniform prior\n",
    "\n",
    "    W = _get_W(f, Z, mu, covInv, eps=eps)\n",
    "\n",
    "    Wt = np.array(W.T, order='C')\n",
    "\n",
    "    for _ in range(em_iter):\n",
    "        denom = np.clip(W @ pi, eps, np.inf)  # Prevent division by zero\n",
    "        update_factor = np.mean(Wt / denom, axis=1)\n",
    "        update_factor = np.nan_to_num(update_factor, nan=1/nsupp, posinf=1/nsupp, neginf=1/nsupp)\n",
    "\n",
    "        pi *= update_factor\n",
    "        pi /= np.sum(pi)  # Normalize to sum to 1\n",
    "\n",
    "    return pi\n",
    "\n",
    "# consider another get W using broadcast\n",
    "# W[i,j] = f(x_i | z_j)\n",
    "def _get_W(f, z, mu, covInv, clip_max=50, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute likelihood matrix W[i, j] = P(X_i | Z_j), ensuring numerical stability.\n",
    "\n",
    "    - Ensures all exponentiation is within safe limits.\n",
    "    - Prevents division by zero.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : ndarray (n_samples, d)\n",
    "        Observed data points.\n",
    "    z : ndarray (n_support, d)\n",
    "        Support points.\n",
    "    mu : ndarray (d, d)\n",
    "        Mean matrix.\n",
    "    covInv : ndarray (d, d)\n",
    "        Inverse covariance matrix.\n",
    "    clip_max : float, optional (default=50)\n",
    "        Maximum absolute value for exponentiation to prevent numerical issues.\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small value to prevent division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W : ndarray (n_samples, n_support)\n",
    "        Likelihood matrix.\n",
    "    \"\"\"\n",
    "    f, z, mu, covInv = [arr.astype(np.float64) for arr in [f, z, mu, covInv]]\n",
    "\n",
    "    fsq = (np.einsum(\"ij,ij->i\", f @ covInv, f) / 2)[:, np.newaxis]  # Shape: (n_samples, 1)\n",
    "    mz = z @ mu.T  # Shape: (n_support, d)\n",
    "    zsq = (np.einsum(\"ij,ij->i\", mz @ covInv, mz) / 2)[:, np.newaxis]  # Shape: (n_support, 1)\n",
    "    \n",
    "    exponent = -fsq - zsq.T + (f @ covInv @ mz.T)  # Shape: (n_samples, n_support)\n",
    "\n",
    "    # Clip exponent to prevent overflow in exp\n",
    "    exponent = np.clip(exponent, -clip_max, clip_max)\n",
    "\n",
    "    # Compute likelihood matrix W and ensure no zero values\n",
    "    W = np.exp(exponent)\n",
    "    W = np.maximum(W, eps)  # Prevent exact zero values\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "# P[i,j] = P(Z_j | X_i)\n",
    "def _get_P(f, z, mu, covInv, pi, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute posterior probability matrix P[i, j] = P(Z_j | X_i).\n",
    "\n",
    "    - Ensures no division by zero.\n",
    "    - Prevents `NaN` values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : ndarray (n_samples, d)\n",
    "        Observed data points.\n",
    "    z : ndarray (n_support, d)\n",
    "        Support points.\n",
    "    mu : ndarray (d, d)\n",
    "        Mean transformation matrix.\n",
    "    covInv : ndarray (d, d)\n",
    "        Inverse covariance matrix.\n",
    "    pi : ndarray (n_support,)\n",
    "        Prior probabilities.\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small positive value to prevent division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    P : ndarray (n_samples, n_support)\n",
    "        Posterior probability matrix.\n",
    "    \"\"\"\n",
    "    W = _get_W(f, z, mu, covInv, eps=eps)\n",
    "    \n",
    "    denom = np.clip(W @ pi, eps, np.inf)  # Prevent division by zero\n",
    "    num = W * pi  # Element-wise multiplication\n",
    "\n",
    "    P = num / denom[:, np.newaxis]  # Normalize probabilities\n",
    "    P = np.nan_to_num(P, nan=1/len(pi))  # Replace NaNs with uniform distribution\n",
    "\n",
    "    return P\n",
    "\n",
    "def _get_P_from_W(W, pi, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute posterior probability matrix P[i, j] = P(Z_j | X_i) from likelihood matrix W.\n",
    "\n",
    "    - Ensures no division by zero.\n",
    "    - Prevents `NaN` values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : ndarray (n_samples, n_support)\n",
    "        Likelihood matrix.\n",
    "    pi : ndarray (n_support,)\n",
    "        Prior probabilities.\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small positive value to prevent division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    P : ndarray (n_samples, n_support)\n",
    "        Posterior probability matrix.\n",
    "    \"\"\"\n",
    "    denom = np.clip(W @ pi, eps, np.inf)  # Prevent division by zero\n",
    "    num = W * pi  # Element-wise multiplication\n",
    "\n",
    "    P = num / denom[:, np.newaxis]  \n",
    "    P = np.nan_to_num(P, nan=1/len(pi))  # Replace NaNs with uniform distribution\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "matrix_outer = lambda A, B: np.einsum(\"bi,bo->bio\", A, B)\n",
    "\n",
    "class _BaseEmpiricalBayes(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for Empirical Bayes estimation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    estimate_prior(f, mu, cov):\n",
    "        Abstract method to estimate the prior distribution.\n",
    "    denoise(f, mu, cov):\n",
    "        Abstract method to denoise posterior observations.\n",
    "    ddenoise(f, mu, cov):\n",
    "        Abstract method to compute the derivative of the denoising function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def estimate_prior(self, f, mu, cov):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def denoise(self, f, mu, cov):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def ddenoise(self, f, mu, cov):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NonparEB(_BaseEmpiricalBayes):\n",
    "    \"\"\"\n",
    "    NPMLE-based empirical Bayes (only supports EM optimizer).\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    estimate_prior(f, mu, cov):\n",
    "        Estimates the prior distribution using the EM algorithm.\n",
    "    denoise(f, mu, cov):\n",
    "        Computes the posterior mean estimates with NaN handling.\n",
    "    ddenoise(f, mu, cov):\n",
    "        Computes the derivative of the denoising function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_nsupp=2000, nsupp_ratio=1, em_iter=500):\n",
    "        super().__init__()\n",
    "        self.em_iter = em_iter\n",
    "        self.nsupp_ratio = nsupp_ratio\n",
    "        self.max_nsupp = max_nsupp\n",
    "        self.pi = None\n",
    "        self.Z = None\n",
    "\n",
    "    def _check_init(self, f, mu):\n",
    "        self.rank = mu.shape[1]\n",
    "        self.nsample = f.shape[0]\n",
    "        self.nsupp = min(int(self.nsupp_ratio * self.nsample), self.max_nsupp or float('inf'))\n",
    "        self.pi = np.full((self.nsupp,), 1 / self.nsupp)\n",
    "\n",
    "        # Compute support points (Z) with `pinv` for stability\n",
    "        if self.nsupp_ratio >= 1:\n",
    "            self.Z = f @ np.linalg.pinv(mu).T\n",
    "        else:\n",
    "            idx = np.random.choice(f.shape[0], self.nsupp, replace=False)\n",
    "            self.Z = f[idx, :] @ np.linalg.pinv(mu).T\n",
    "\n",
    "    def estimate_prior(self, f, mu, cov):\n",
    "        self._check_init(f, mu)\n",
    "        covInv = np.linalg.pinv(cov)  # Use pseudo-inverse for stability\n",
    "        self.pi = _npmle_em_hd(f, self.Z, mu, covInv, self.em_iter)\n",
    "        return self.Z, self.pi  # Return support points and probability weights\n",
    "\n",
    "    def denoise(self, f, mu, cov, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Compute the denoised posterior estimates with NaN handling.\n",
    "\n",
    "        - Ensures rows with NaN values are replaced by prior mean.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : ndarray (n, d)\n",
    "            Observed data points.\n",
    "        mu : ndarray (d, d)\n",
    "            Mean transformation matrix.\n",
    "        cov : ndarray (d, d)\n",
    "            Covariance matrix of the prior.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        denoised_values : ndarray (n, d)\n",
    "            Posterior mean estimates with NaN handling.\n",
    "        \"\"\"\n",
    "        covInv = np.linalg.pinv(cov)  # Use pseudo-inverse for numerical stability\n",
    "        P = _get_P(f, self.Z, mu, covInv, self.pi)\n",
    "\n",
    "        denoised_values = P @ self.Z  # Compute posterior mean\n",
    "\n",
    "        # Identify rows with NaN values and replace them with prior mean\n",
    "        nan_rows = np.isnan(denoised_values).any(axis=1)\n",
    "        if np.any(nan_rows):\n",
    "            warnings.warn(\"NaN detected in denoised output. Replacing affected rows with prior mean.\", RuntimeWarning)\n",
    "            denoised_values[nan_rows] = np.average(self.Z, axis=0, weights=self.pi)\n",
    "\n",
    "        return np.nan_to_num(denoised_values, nan=0.0)  # Ensure no NaNs remain\n",
    "\n",
    "    def ddenoise(self, f, mu, cov):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the denoising function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : ndarray (n, d)\n",
    "            Observed data points.\n",
    "        mu : ndarray (d, d)\n",
    "            Mean transformation matrix.\n",
    "        cov : ndarray (d, d)\n",
    "            Covariance matrix of the prior.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative : ndarray\n",
    "            The derivative of the denoising function at the posterior observations.\n",
    "        \"\"\"\n",
    "        covInv = np.linalg.pinv(cov)\n",
    "        P = _get_P(f, self.Z, mu, covInv, self.pi)\n",
    "        ZouterMZ = np.einsum(\"ijk, kl -> ijl\", matrix_outer(self.Z, self.Z @ mu.T), covInv)\n",
    "        E1 = np.einsum(\"ij, jkl -> ikl\", P, ZouterMZ)\n",
    "        E2a = P @ self.Z\n",
    "        E2 = np.einsum(\"ijk, kl -> ijl\", matrix_outer(E2a, E2a @ mu.T), covInv)\n",
    "\n",
    "        return E1 - E2\n",
    "\n",
    "class NonparBayes(NonparEB):\n",
    "    \"\"\"\n",
    "    Nonparametric Bayes with a Known Prior.\n",
    "\n",
    "    This class extends `NonparEB` but does not estimate a prior from data. \n",
    "    Instead, it takes a known prior (locations and weights) as input.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Z : ndarray\n",
    "        The known prior locations (support points).\n",
    "    pi : ndarray\n",
    "        The weights associated with the prior locations.\n",
    "    rank : int\n",
    "        Dimensionality of the prior distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, truePriorLoc, truePriorWeight=None):\n",
    "        \"\"\"\n",
    "        Initialize Nonparametric Bayes model with a known prior.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        truePriorLoc : ndarray of shape (n, k)\n",
    "            The known prior locations, where `n` is the number of support points \n",
    "            and `k` is the dimensionality.\n",
    "        truePriorWeight : ndarray of shape (n,), optional\n",
    "            The probability weights associated with `truePriorLoc`. If not provided, \n",
    "            a uniform distribution over `n` points is assumed.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the provided prior locations and weights do not match dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure `truePriorLoc` is a 2D array\n",
    "        if truePriorLoc.ndim != 2:\n",
    "            raise ValueError(\"truePriorLoc must be a 2D array of shape (n, k)\")\n",
    "\n",
    "        n, k = truePriorLoc.shape\n",
    "\n",
    "        self.Z = truePriorLoc.astype(np.float64)  # Ensure numerical stability\n",
    "        self.rank = k  # Dimensionality of the prior distribution\n",
    "\n",
    "        # Store prior weights (uniform if not provided)\n",
    "        if truePriorWeight is None:\n",
    "            self.pi = np.full((n,), 1 / n, dtype=np.float64)\n",
    "        else:\n",
    "            if truePriorWeight.ndim != 1:\n",
    "                raise ValueError(\"truePriorWeight must be a 1D array of shape (n,)\")\n",
    "            if truePriorWeight.shape[0] != n:\n",
    "                raise ValueError(f\"truePriorWeight must match truePriorLoc in size ({n},)\")\n",
    "\n",
    "            self.pi = np.array(truePriorWeight, dtype=np.float64)\n",
    "\n",
    "    def estimate_prior(self, f, mu, cov):\n",
    "        \"\"\"\n",
    "        No prior estimation is needed since the prior is already given.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ClusterEmpiricalBayes:\n",
    "    \"\"\"\n",
    "    Handles clustering of modalities, aggregation of data, and estimation of empirical Bayes priors.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cluster_data : dict\n",
    "        Maps each cluster to its concatenated data matrix.\n",
    "    cluster_M : dict\n",
    "        Maps each cluster to its block-diagonal M matrix.\n",
    "    cluster_S : dict\n",
    "        Maps each cluster to its block-diagonal S matrix.\n",
    "    cluster_priors : dict\n",
    "        Maps each cluster to (support_points, prior_weights).\n",
    "    modality_denoisers : dict\n",
    "        Maps each modality index to a function that extracts its denoised values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_matrices, M_matrices, S_matrices, cluster_labels):\n",
    "        \"\"\"\n",
    "        Initialize the ClusterEmpiricalBayes class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_matrices : list of ndarrays\n",
    "            List of m data matrices X_k of shape (n, r_k).\n",
    "        M_matrices : list of ndarrays\n",
    "            List of m transformation matrices M_k of shape (r_k, p_k).\n",
    "        S_matrices : list of ndarrays\n",
    "            List of m noise matrices S_k of shape (r_k, r_k).\n",
    "        cluster_labels : list or ndarray\n",
    "            Cluster labels of length m, indicating the cluster index for each modality.\n",
    "        \"\"\"\n",
    "        if not (len(data_matrices) == len(M_matrices) == len(S_matrices) == len(cluster_labels)):\n",
    "            raise ValueError(\"Mismatch in number of modalities among data_matrices, M_matrices, S_matrices, and cluster_labels.\")\n",
    "\n",
    "        self.data_matrices = data_matrices  # Store raw data per modality\n",
    "        self.cluster_labels = cluster_labels  # Store cluster assignments for modalities\n",
    "\n",
    "        # Aggregate cluster data\n",
    "        self.cluster_data, self.cluster_M, self.cluster_S = self.aggregate_cluster_data(\n",
    "            data_matrices, M_matrices, S_matrices, cluster_labels\n",
    "        )\n",
    "\n",
    "        # Dictionary to store cluster priors\n",
    "        self.cluster_priors = {}\n",
    "\n",
    "        # Dictionary to store denoising functions for each modality\n",
    "        self.modality_denoisers = {}\n",
    "\n",
    "    def aggregate_cluster_data(self, data_matrices, M_matrices, S_matrices, cluster_labels):\n",
    "        \"\"\"\n",
    "        Aggregates data, M, and S matrices based on cluster labels and constructs block-diagonal M and S.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_data, cluster_M, cluster_S : dict\n",
    "            Dictionaries mapping each cluster index to its aggregated data, block-diagonal M, and block-diagonal S.\n",
    "        \"\"\"\n",
    "        cluster_data = defaultdict(list)\n",
    "        cluster_M = defaultdict(list)\n",
    "        cluster_S = defaultdict(list)\n",
    "\n",
    "        for k, cluster in enumerate(cluster_labels):\n",
    "            cluster_data[cluster].append(data_matrices[k].astype(np.float64))  # Ensure numerical stability\n",
    "            cluster_M[cluster].append(M_matrices[k].astype(np.float64))\n",
    "            cluster_S[cluster].append(S_matrices[k].astype(np.float64))\n",
    "\n",
    "        for cluster in cluster_data:\n",
    "            sample_sizes = [X.shape[0] for X in cluster_data[cluster]]\n",
    "            if len(set(sample_sizes)) > 1:\n",
    "                raise ValueError(f\"Mismatch in sample sizes for cluster {cluster}: {sample_sizes}\")\n",
    "\n",
    "            cluster_data[cluster] = np.concatenate(cluster_data[cluster], axis=1)\n",
    "            cluster_M[cluster] = block_diag(*cluster_M[cluster])\n",
    "            cluster_S[cluster] = block_diag(*cluster_S[cluster])\n",
    "\n",
    "        return cluster_data, cluster_M, cluster_S\n",
    "\n",
    "    def estimate_cluster_priors(self, em_iter=500, nsupp_ratio=0.5, max_nsupp=100):\n",
    "        \"\"\"\n",
    "        Estimates priors (per-cluster) and denoisers (per-modality) using Nonparametric Empirical Bayes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_priors : dict\n",
    "            Dictionary mapping each cluster to (support_points, prior_weights).\n",
    "        modality_denoisers : dict\n",
    "            Dictionary mapping each modality index to a function that extracts its denoised values.\n",
    "        \"\"\"\n",
    "        cluster_denoisers = {}\n",
    "\n",
    "        # Estimate priors at the cluster level\n",
    "        for cluster in self.cluster_data:\n",
    "            X_cluster = self.cluster_data[cluster]\n",
    "            M_cluster = self.cluster_M[cluster]\n",
    "            S_cluster = self.cluster_S[cluster]\n",
    "\n",
    "            if X_cluster.shape[1] != M_cluster.shape[0]:\n",
    "                raise ValueError(f\"Mismatch in dimensions for cluster {cluster}: X ({X_cluster.shape}) and M ({M_cluster.shape})\")\n",
    "            if S_cluster.shape[0] != S_cluster.shape[1]:\n",
    "                raise ValueError(f\"Noise matrix S for cluster {cluster} is not square: {S_cluster.shape}\")\n",
    "            if S_cluster.shape[0] != M_cluster.shape[0]:\n",
    "                raise ValueError(f\"Mismatch in S ({S_cluster.shape}) and M ({M_cluster.shape}) for cluster {cluster}\")\n",
    "\n",
    "            # Estimate prior using empirical Bayes\n",
    "            nonpar_eb = NonparEB(em_iter=em_iter, nsupp_ratio=nsupp_ratio, max_nsupp=max_nsupp)\n",
    "            support_points, prior_weights = nonpar_eb.estimate_prior(X_cluster, M_cluster, S_cluster)\n",
    "\n",
    "            # Store prior per cluster\n",
    "            self.cluster_priors[cluster] = (support_points, prior_weights)\n",
    "            cluster_denoisers[cluster] = nonpar_eb  # Store corresponding denoiser object\n",
    "\n",
    "        # Define denoisers per modality\n",
    "        for modality_idx, cluster in enumerate(self.cluster_labels):\n",
    "            nonpar_eb = cluster_denoisers[cluster]  # Use cluster-specific prior\n",
    "\n",
    "            def create_denoise_func(nonpar_eb):\n",
    "                return lambda f, mu, cov: nonpar_eb.denoise(f, mu, cov)\n",
    "\n",
    "            def create_ddenoise_func(nonpar_eb):\n",
    "                return lambda f, mu, cov: nonpar_eb.ddenoise(f, mu, cov)\n",
    "\n",
    "            self.modality_denoisers[modality_idx] = (\n",
    "                create_denoise_func(nonpar_eb),\n",
    "                create_ddenoise_func(nonpar_eb),\n",
    "            )\n",
    "\n",
    "        return self.cluster_priors, self.modality_denoisers\n",
    "\n",
    "def generate_synthetic_data(num_modalities=6, num_clusters=3, n=100, r_range=(3, 7), noise_scale=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data matrices X_k = M_k U_k + S_k^{1/2} Z_k with cluster-correlated latent factors.\n",
    "\n",
    "    - Ensures numerical stability (`np.float64`).\n",
    "    - Prevents zero or negative values in noise covariance matrices.\n",
    "    - Avoids `NaN` values in generated data.\n",
    "    - Ensures each cluster has at least one modality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_modalities : int\n",
    "        Number of different modalities.\n",
    "    num_clusters : int\n",
    "        Number of clusters.\n",
    "    n : int\n",
    "        Number of samples (same across all modalities).\n",
    "    r_range : tuple\n",
    "        Range of values for r_k (dimensionality of each modality).\n",
    "    noise_scale : float\n",
    "        Standard deviation of noise components.\n",
    "    seed : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_matrices : list of ndarrays\n",
    "        Generated X_k data matrices of different dimensions.\n",
    "    M_matrices : list of ndarrays\n",
    "        Transformation matrices M_k of different sizes.\n",
    "    S_matrices : list of ndarrays\n",
    "        Diagonal noise matrices S_k of different sizes.\n",
    "    cluster_labels : ndarray\n",
    "        Cluster assignments for each modality.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)  # Set seed for reproducibility\n",
    "\n",
    "    # Assign each modality to a cluster\n",
    "    cluster_labels = np.random.randint(0, num_clusters, size=num_modalities).astype(np.int64)\n",
    "\n",
    "    # Determine r_k for each modality, ensuring positive dimensions\n",
    "    modality_dims = np.random.randint(r_range[0], r_range[1] + 1, size=num_modalities)\n",
    "\n",
    "    # Ensure each cluster has at least one modality\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    \n",
    "    # Compute the minimum r_c for each cluster\n",
    "    cluster_min_dims = {c: min(modality_dims[cluster_labels == c]) for c in unique_clusters}\n",
    "\n",
    "    # Generate shared cluster-wise latent variables U_c\n",
    "    cluster_latents = {c: np.random.randn(n, cluster_min_dims[c]).astype(np.float64) for c in unique_clusters}\n",
    "\n",
    "    # Initialize lists for data matrices, transformation matrices, and noise matrices\n",
    "    data_matrices = []\n",
    "    M_matrices = []\n",
    "    S_matrices = []\n",
    "\n",
    "    for k in range(num_modalities):\n",
    "        r_k = modality_dims[k]  # Dimension of modality k\n",
    "        cluster_idx = cluster_labels[k]  # Assigned cluster\n",
    "        r_c = cluster_min_dims[cluster_idx]  # Minimum r_c in the cluster\n",
    "\n",
    "        # Generate `U_k`, ensuring numerical stability\n",
    "        U_k = np.hstack([\n",
    "            cluster_latents[cluster_idx],  # First r_c columns from U_c\n",
    "            np.random.randn(n, r_k - r_c).astype(np.float64) if r_k > r_c else np.empty((n, 0))\n",
    "        ])\n",
    "\n",
    "        # Generate transformation matrix M_k (r_k × r_k), ensuring no zero diagonal elements\n",
    "        M_k = np.diag(np.random.uniform(0.5, 1.5, size=r_k).astype(np.float64))\n",
    "\n",
    "        # Generate noise matrix S_k (diagonal, r_k × r_k) with minimum values\n",
    "        S_k_diag = np.clip(np.random.uniform(0.05, noise_scale, size=r_k), 1e-4, np.inf)\n",
    "        S_k = np.diag(S_k_diag).astype(np.float64)\n",
    "\n",
    "        # Generate noise Z_k, ensuring numerical stability\n",
    "        Z_k = np.random.randn(n, r_k).astype(np.float64)\n",
    "\n",
    "        # Compute X_k = M_k U_k + S_k^{1/2} Z_k\n",
    "        X_k = (U_k @ M_k.T) + (Z_k @ np.sqrt(S_k))\n",
    "\n",
    "        # Store results\n",
    "        data_matrices.append(X_k)\n",
    "        M_matrices.append(M_k)\n",
    "        S_matrices.append(S_k)\n",
    "\n",
    "    return data_matrices, M_matrices, S_matrices, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate synthetic data with cluster-correlated modalities\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_matrices, M_matrices, S_matrices, cluster_labels \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_synthetic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_modalities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print cluster assignments\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster Labels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cluster_labels)\n",
      "Cell \u001b[0;32mIn[4], line 539\u001b[0m, in \u001b[0;36mgenerate_synthetic_data\u001b[0;34m(num_modalities, num_clusters, n, r_range, noise_scale, seed)\u001b[0m\n\u001b[1;32m    536\u001b[0m modality_dims \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(r_range[\u001b[38;5;241m0\u001b[39m], r_range[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39mnum_modalities)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# Compute the minimum r_c for each cluster\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m cluster_min_dims \u001b[38;5;241m=\u001b[39m {c: \u001b[38;5;28mmin\u001b[39m(modality_dims[cluster_labels \u001b[38;5;241m==\u001b[39m c]) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43munique_clusters\u001b[49m}\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Generate shared cluster-wise latent variables U_c\u001b[39;00m\n\u001b[1;32m    542\u001b[0m cluster_latents \u001b[38;5;241m=\u001b[39m {c: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(n, cluster_min_dims[c])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m unique_clusters}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_clusters' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data with cluster-correlated modalities\n",
    "data_matrices, M_matrices, S_matrices, cluster_labels = generate_synthetic_data(num_modalities=6, num_clusters=3, seed = 10)\n",
    "\n",
    "# Print cluster assignments\n",
    "print(\"Cluster Labels:\", cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical_amp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
