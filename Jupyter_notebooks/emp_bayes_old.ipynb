{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.linalg import block_diag\n",
    "from abc import ABC, abstractmethod\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _npmle_em_hd(f, Z, mu, covInv, em_iter):\n",
    "    # I dont need this n_dim\n",
    "    nsupp = Z.shape[0]\n",
    "    pi = np.array([1/nsupp] * nsupp)\n",
    "    W = _get_W(f, Z, mu, covInv)\n",
    "\n",
    "    Wt = np.array(W.T, order = 'C')\n",
    "    for _ in range(em_iter):\n",
    "        denom = W.dot(pi)# [:,np.newaxis] # denom[i] = \\sum_j pi[j]*W[i,j]\n",
    "        pi = pi * np.mean(Wt/denom, axis = 1)\n",
    "    \n",
    "    return pi\n",
    "\n",
    "# consider another get W using broadcast\n",
    "# W[i,j] = f(x_i | z_j)\n",
    "def _get_W(f, z, mu, covInv):\n",
    "    fsq = (np.einsum(\"ik,ik -> i\", f @ covInv, f) / 2)[:,np.newaxis]\n",
    "    mz = z.dot(mu.T)\n",
    "    zsq = np.einsum(\"ik, ik->i\", mz @ covInv, mz) / 2\n",
    "    fz = f @ covInv @ mz.T\n",
    "    del mz\n",
    "    return np.exp(- fsq + fz - zsq)\n",
    "\n",
    "# P[i,j] = P(Z_j | X_i)\n",
    "def _get_P(f,z,mu,covInv,pi):\n",
    "    W = _get_W(f,z,mu,covInv)\n",
    "    denom = W.dot(pi) # denom[i] = \\sum_j pi[j] * W[i,j]\n",
    "    num = W * pi # W*pi[i,j] = pi[j] * W[i,j]\n",
    "    return num / denom[:, np.newaxis]\n",
    "\n",
    "def _get_P_from_W(W, pi):\n",
    "    denom = W.dot(pi) # denom[i] = \\sum_j pi[j] * W[i,j]\n",
    "    num = W * pi # W*pi[i,j] = pi[j] * W[i,j]\n",
    "    return num / denom[:, np.newaxis]\n",
    "\n",
    "\n",
    "matrix_outer = lambda A, B: np.einsum(\"bi,bo->bio\", A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseEmpiricalBayes(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for Empirical Bayes estimation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    estimate_prior(f, mu, cov):\n",
    "        Abstract method to estimate the prior distribution.\n",
    "    denoise(f, mu, cov):\n",
    "        Abstract method to denoise posterior observations.\n",
    "    ddenoise(f, mu, cov):\n",
    "        Abstract method to compute the derivative of the denoising function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def estimate_prior(self, f, mu, cov):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def denoise(self, f, mu, cov):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def ddenoise(self, f, mu, cov):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NonparEB(_BaseEmpiricalBayes):\n",
    "    \"\"\"\n",
    "    NPMLE-based empirical Bayes (only supports EM optimizer).\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    estimate_prior(f, mu, cov):\n",
    "        Estimates the prior distribution using the EM algorithm.\n",
    "    denoise(f, mu, cov):\n",
    "        Computes the posterior mean estimates.\n",
    "    ddenoise(f, mu, cov):\n",
    "        Computes the derivative of the denoising function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_nsupp=2000, nsupp_ratio=1, em_iter=500):\n",
    "        super().__init__()\n",
    "        self.em_iter = em_iter\n",
    "        self.nsupp_ratio = nsupp_ratio\n",
    "        self.max_nsupp = max_nsupp\n",
    "        self.pi = None\n",
    "        self.Z = None\n",
    "\n",
    "    def _check_init(self, f, mu):\n",
    "        self.rank = mu.shape[1]\n",
    "        self.nsample = f.shape[0]\n",
    "        self.nsupp = min(int(self.nsupp_ratio * self.nsample), self.max_nsupp or float('inf'))\n",
    "        self.pi = np.full((self.nsupp,), 1 / self.nsupp)\n",
    "\n",
    "        # Compute support points (Z)\n",
    "        if self.nsupp_ratio >= 1:\n",
    "            self.Z = f @ np.linalg.pinv(mu).T\n",
    "        else:\n",
    "            idx = np.random.choice(f.shape[0], self.nsupp, replace=False)\n",
    "            self.Z = f[idx, :] @ np.linalg.pinv(mu).T\n",
    "\n",
    "    def estimate_prior(self, f, mu, cov):\n",
    "        self._check_init(f, mu)\n",
    "        covInv = np.linalg.inv(cov)\n",
    "        self.pi = _npmle_em_hd(f, self.Z, mu, covInv, self.em_iter)\n",
    "        return self.Z, self.pi  # Return support points and probability weights\n",
    "\n",
    "    def denoise(self, f, mu, cov):\n",
    "        \"\"\"\n",
    "        Compute the denoised posterior estimates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : ndarray (n, d)\n",
    "            Observed data points.\n",
    "        mu : ndarray (d, d)\n",
    "            Mean transformation matrix.\n",
    "        cov : ndarray (d, d)\n",
    "            Covariance matrix of the prior.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        denoised_values : ndarray (n, d)\n",
    "            Posterior mean estimates.\n",
    "        \"\"\"\n",
    "        covInv = np.linalg.inv(cov)\n",
    "        P = _get_P(f, self.Z, mu, covInv, self.pi)\n",
    "        return P @ self.Z\n",
    "\n",
    "    def ddenoise(self, f, mu, cov):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the denoising function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : ndarray (n, d)\n",
    "            Observed data points.\n",
    "        mu : ndarray (d, d)\n",
    "            Mean transformation matrix.\n",
    "        cov : ndarray (d, d)\n",
    "            Covariance matrix of the prior.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative : ndarray\n",
    "            The derivative of the denoising function at the posterior observations.\n",
    "        \"\"\"\n",
    "        covInv = np.linalg.inv(cov)\n",
    "        P = _get_P(f, self.Z, mu, covInv, self.pi)\n",
    "        ZouterMZ = np.einsum(\"ijk, kl -> ijl\", matrix_outer(self.Z, self.Z @ mu.T), covInv)\n",
    "        E1 = np.einsum(\"ij, jkl -> ikl\", P, ZouterMZ)\n",
    "        E2a = P @ self.Z\n",
    "        E2 = np.einsum(\"ijk, kl -> ijl\", matrix_outer(E2a, E2a @ mu.T), covInv)\n",
    "        return E1 - E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonparBayes(NonparEB):\n",
    "    \"\"\"\n",
    "    Nonparametric Bayes with a Known Prior.\n",
    "\n",
    "    This class extends `NonparEB` but does not estimate a prior from data. \n",
    "    Instead, it takes a known prior (locations and weights) as input.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Z : ndarray\n",
    "        The known prior locations (support points).\n",
    "    pi : ndarray\n",
    "        The weights associated with the prior locations.\n",
    "    rank : int\n",
    "        Dimensionality of the prior distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, truePriorLoc, truePriorWeight=None):\n",
    "        \"\"\"\n",
    "        Initialize Nonparametric Bayes model with a known prior.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        truePriorLoc : ndarray of shape (n, k)\n",
    "            The known prior locations, where `n` is the number of support points \n",
    "            and `k` is the dimensionality.\n",
    "        truePriorWeight : ndarray of shape (n,), optional\n",
    "            The probability weights associated with `truePriorLoc`. If not provided, \n",
    "            a uniform distribution over `n` points is assumed.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the provided prior locations and weights do not match dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure `truePriorLoc` is a 2D array\n",
    "        if truePriorLoc.ndim != 2:\n",
    "            raise ValueError(\"truePriorLoc must be a 2D array of shape (n, k)\")\n",
    "\n",
    "        n, k = truePriorLoc.shape\n",
    "\n",
    "        self.Z = truePriorLoc  # Store prior locations\n",
    "        self.rank = k  # Dimensionality of the prior distribution\n",
    "\n",
    "        # Store prior weights (uniform if not provided)\n",
    "        if truePriorWeight is None:\n",
    "            self.pi = np.full((n,), 1 / n)\n",
    "        else:\n",
    "            if truePriorWeight.ndim != 1:\n",
    "                raise ValueError(\"truePriorWeight must be a 1D array of shape (n,)\")\n",
    "            if truePriorWeight.shape[0] != n:\n",
    "                raise ValueError(f\"truePriorWeight must match truePriorLoc in size ({n},)\")\n",
    "\n",
    "            self.pi = truePriorWeight\n",
    "\n",
    "    def estimate_prior(self, f, mu, cov):\n",
    "        \"\"\"\n",
    "        No prior estimation is needed since the prior is already given.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterEmpiricalBayes:\n",
    "    \"\"\"\n",
    "    Handles clustering of modalities, aggregation of data, and estimation of empirical Bayes priors.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cluster_data : dict\n",
    "        Maps each cluster to its concatenated data matrix.\n",
    "    cluster_M : dict\n",
    "        Maps each cluster to its block-diagonal M matrix.\n",
    "    cluster_S : dict\n",
    "        Maps each cluster to its block-diagonal S matrix.\n",
    "    cluster_priors : dict\n",
    "        Maps each cluster to (support_points, prior_weights).\n",
    "    modality_denoisers : dict\n",
    "        Maps each modality index to a function that extracts its denoised values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_matrices, M_matrices, S_matrices, cluster_labels):\n",
    "        \"\"\"\n",
    "        Initialize the ClusterEmpiricalBayes class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_matrices : list of ndarrays\n",
    "            List of m data matrices X_k of shape (n, r_k).\n",
    "        M_matrices : list of ndarrays\n",
    "            List of m transformation matrices M_k of shape (r_k, p_k).\n",
    "        S_matrices : list of ndarrays\n",
    "            List of m noise matrices S_k of shape (r_k, r_k).\n",
    "        cluster_labels : list or ndarray\n",
    "            Cluster labels of length m, indicating the cluster index for each modality.\n",
    "        \"\"\"\n",
    "        if not (len(data_matrices) == len(M_matrices) == len(S_matrices) == len(cluster_labels)):\n",
    "            raise ValueError(\"Mismatch in number of modalities among data_matrices, M_matrices, S_matrices, and cluster_labels.\")\n",
    "\n",
    "        self.data_matrices = data_matrices  # Store raw data per modality\n",
    "        self.cluster_labels = cluster_labels  # Store cluster assignments for modalities\n",
    "\n",
    "        # Aggregate cluster data\n",
    "        self.cluster_data, self.cluster_M, self.cluster_S = self.aggregate_cluster_data(\n",
    "            data_matrices, M_matrices, S_matrices, cluster_labels\n",
    "        )\n",
    "\n",
    "        # Dictionary to store cluster priors\n",
    "        self.cluster_priors = {}\n",
    "\n",
    "        # Dictionary to store denoising functions for each modality\n",
    "        self.modality_denoisers = {}\n",
    "\n",
    "    def aggregate_cluster_data(self, data_matrices, M_matrices, S_matrices, cluster_labels):\n",
    "        \"\"\"\n",
    "        Aggregates data, M, and S matrices based on cluster labels and constructs block-diagonal M and S.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_data, cluster_M, cluster_S : dict\n",
    "            Dictionaries mapping each cluster index to its aggregated data, block-diagonal M, and block-diagonal S.\n",
    "        \"\"\"\n",
    "        cluster_data = defaultdict(list)\n",
    "        cluster_M = defaultdict(list)\n",
    "        cluster_S = defaultdict(list)\n",
    "\n",
    "        for k, cluster in enumerate(cluster_labels):\n",
    "            cluster_data[cluster].append(data_matrices[k])  # Append data X_k\n",
    "            cluster_M[cluster].append(M_matrices[k])  # Append M_k\n",
    "            cluster_S[cluster].append(S_matrices[k])  # Append S_k\n",
    "\n",
    "        for cluster in cluster_data:\n",
    "            sample_sizes = [X.shape[0] for X in cluster_data[cluster]]\n",
    "            if len(set(sample_sizes)) > 1:\n",
    "                raise ValueError(f\"Mismatch in sample sizes for cluster {cluster}: {sample_sizes}\")\n",
    "\n",
    "            cluster_data[cluster] = np.concatenate(cluster_data[cluster], axis=1)\n",
    "            cluster_M[cluster] = block_diag(*cluster_M[cluster])\n",
    "            cluster_S[cluster] = block_diag(*cluster_S[cluster])\n",
    "\n",
    "        return cluster_data, cluster_M, cluster_S\n",
    "\n",
    "    def estimate_cluster_priors(self, em_iter=500, nsupp_ratio=0.5, max_nsupp=100):\n",
    "        \"\"\"\n",
    "        Estimates priors (per-cluster) and denoisers (per-modality) using Nonparametric Empirical Bayes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_priors : dict\n",
    "            Dictionary mapping each cluster to (support_points, prior_weights).\n",
    "        modality_denoisers : dict\n",
    "            Dictionary mapping each modality index to a function that extracts its denoised values.\n",
    "        \"\"\"\n",
    "        cluster_denoisers = {}\n",
    "\n",
    "        # Estimate priors at the cluster level\n",
    "        for cluster in self.cluster_data:\n",
    "            X_cluster = self.cluster_data[cluster]\n",
    "            M_cluster = self.cluster_M[cluster]\n",
    "            S_cluster = self.cluster_S[cluster]\n",
    "\n",
    "            if X_cluster.shape[1] != M_cluster.shape[0]:\n",
    "                raise ValueError(f\"Mismatch in dimensions for cluster {cluster}: X ({X_cluster.shape}) and M ({M_cluster.shape})\")\n",
    "            if S_cluster.shape[0] != S_cluster.shape[1]:\n",
    "                raise ValueError(f\"Noise matrix S for cluster {cluster} is not square: {S_cluster.shape}\")\n",
    "            if S_cluster.shape[0] != M_cluster.shape[0]:\n",
    "                raise ValueError(f\"Mismatch in S ({S_cluster.shape}) and M ({M_cluster.shape}) for cluster {cluster}\")\n",
    "\n",
    "            # Estimate prior using empirical Bayes\n",
    "            nonpar_eb = NonparEB(em_iter=em_iter, nsupp_ratio=nsupp_ratio, max_nsupp=max_nsupp)\n",
    "            support_points, prior_weights = nonpar_eb.estimate_prior(X_cluster, M_cluster, S_cluster)\n",
    "\n",
    "            # Store prior per cluster\n",
    "            self.cluster_priors[cluster] = (support_points, prior_weights)\n",
    "            cluster_denoisers[cluster] = nonpar_eb  # Store corresponding denoiser object\n",
    "\n",
    "        # Define denoisers per modality\n",
    "        for modality_idx, cluster in enumerate(self.cluster_labels):\n",
    "            start_col = sum(\n",
    "                self.data_matrices[i].shape[1] for i in range(modality_idx) if self.cluster_labels[i] == cluster\n",
    "            )\n",
    "            end_col = start_col + self.data_matrices[modality_idx].shape[1]\n",
    "\n",
    "            nonpar_eb = cluster_denoisers[cluster]  # Use cluster-specific prior\n",
    "\n",
    "            def create_denoise_func(nonpar_eb, start_col, end_col):\n",
    "                def denoise_func(f, mu, cov):\n",
    "                    denoised_cluster = nonpar_eb.denoise(f, mu, cov)\n",
    "                    return denoised_cluster[:, start_col:end_col]\n",
    "                return denoise_func\n",
    "\n",
    "            def create_ddenoise_func(nonpar_eb, start_col, end_col):\n",
    "                def ddenoise_func(f, mu, cov):\n",
    "                    ddenoised_cluster = nonpar_eb.ddenoise(f, mu, cov)\n",
    "                    return ddenoised_cluster[:, start_col:end_col, start_col:end_col]\n",
    "                return ddenoise_func\n",
    "\n",
    "            self.modality_denoisers[modality_idx] = (\n",
    "                create_denoise_func(nonpar_eb, start_col, end_col),\n",
    "                create_ddenoise_func(nonpar_eb, start_col, end_col),\n",
    "            )\n",
    "\n",
    "        return self.cluster_priors, self.modality_denoisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_modalities=6, num_clusters=3, n=100, r_range=(3, 7), noise_scale=0.1, seed = 42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data matrices X_k = M_k U_k + S_k^{1/2} Z_k with cluster-correlated latent factors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_modalities : int\n",
    "        Number of different modalities.\n",
    "    num_clusters : int\n",
    "        Number of clusters.\n",
    "    n : int\n",
    "        Number of samples (same across all modalities).\n",
    "    r_range : tuple\n",
    "        Range of values for r_k (dimensionality of each modality).\n",
    "    noise_scale : float\n",
    "        Standard deviation of noise components.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_matrices : list of ndarrays\n",
    "        Generated X_k data matrices of different dimensions.\n",
    "    M_matrices : list of ndarrays\n",
    "        Transformation matrices M_k of different sizes.\n",
    "    S_matrices : list of ndarrays\n",
    "        Diagonal noise matrices S_k of different sizes.\n",
    "    cluster_labels : ndarray\n",
    "        Cluster assignments for each modality.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)  # For reproducibility\n",
    "\n",
    "    # Assign modalities to clusters\n",
    "    cluster_labels = np.random.randint(0, num_clusters, size=num_modalities)\n",
    "    cluster_labels = np.array(cluster_labels)  # Ensure it's a NumPy array\n",
    "\n",
    "    # Determine r_k for each modality\n",
    "    modality_dims = np.random.randint(*r_range, size=num_modalities)\n",
    "\n",
    "    # Ensure each cluster has at least one modality\n",
    "    existing_clusters = np.unique(cluster_labels)  # Only clusters with assigned modalities\n",
    "\n",
    "    # Find the minimum r_c for each existing cluster\n",
    "    cluster_min_dims = {\n",
    "        c: min(modality_dims[cluster_labels == c]) for c in existing_clusters\n",
    "    }\n",
    "\n",
    "    # Generate shared cluster-wise latent variables U_c\n",
    "    cluster_latents = {\n",
    "        c: np.random.randn(n, cluster_min_dims[c]) for c in existing_clusters\n",
    "    }\n",
    "\n",
    "    # Generate modality-wise data\n",
    "    data_matrices = []\n",
    "    M_matrices = []\n",
    "    S_matrices = []\n",
    "\n",
    "    for k in range(num_modalities):\n",
    "        r_k = modality_dims[k]  # Dimension of this modality\n",
    "        cluster_idx = cluster_labels[k]  # Get assigned cluster\n",
    "        r_c = cluster_min_dims[cluster_idx]  # Get min cluster dim for this cluster\n",
    "\n",
    "        # Generate `U_k` where first `r_c` columns are from `U_c`, remaining are random\n",
    "        U_k = np.hstack([\n",
    "            cluster_latents[cluster_idx],  # First r_c columns from U_c\n",
    "            np.random.randn(n, r_k - r_c) if r_k > r_c else np.empty((n, 0))  # Additional columns\n",
    "        ])\n",
    "\n",
    "        # Generate transformation matrix M_k (r_k × r_k)\n",
    "        M_k = np.diag(np.random.uniform(0.5, 1.5, size=r_k))\n",
    "\n",
    "        # Generate noise matrix S_k (diagonal, r_k × r_k)\n",
    "        S_k = np.diag(np.random.uniform(0.05, noise_scale, size=r_k))\n",
    "\n",
    "        # Generate noise Z_k\n",
    "        Z_k = np.random.randn(n, r_k)\n",
    "\n",
    "        # Compute X_k = M_k U_k + S_k^{1/2} Z_k\n",
    "        X_k = U_k @ M_k.T + Z_k @ np.sqrt(S_k)\n",
    "\n",
    "        # Store results\n",
    "        data_matrices.append(X_k)\n",
    "        M_matrices.append(M_k)\n",
    "        S_matrices.append(S_k)\n",
    "\n",
    "    return data_matrices, M_matrices, S_matrices, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data with cluster-correlated modalities\n",
    "data_matrices, M_matrices, S_matrices, cluster_labels = generate_synthetic_data(num_modalities=6, num_clusters=3, seed = 10)\n",
    "\n",
    "# Print cluster assignments\n",
    "print(\"Cluster Labels:\", cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceb = ClusterEmpiricalBayes(data_matrices, M_matrices, S_matrices, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_priors, modality_denoisers = ceb.estimate_cluster_priors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 1: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 2: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 3: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 4: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 5: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>)}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modality_denoisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      "  - Support Points Shape: (50, 12)\n",
      "  - Prior Weights Shape: (50,)\n",
      "Cluster 0:\n",
      "  - Support Points Shape: (50, 12)\n",
      "  - Prior Weights Shape: (50,)\n",
      "Modality 0 has a denoiser and derivative function.\n",
      "Modality 1 has a denoiser and derivative function.\n",
      "Modality 2 has a denoiser and derivative function.\n",
      "Modality 3 has a denoiser and derivative function.\n",
      "Modality 4 has a denoiser and derivative function.\n",
      "Modality 5 has a denoiser and derivative function.\n"
     ]
    }
   ],
   "source": [
    "for cluster_idx, (support_points, prior_weights) in cluster_priors.items():\n",
    "    print(f\"Cluster {cluster_idx}:\")\n",
    "    print(f\"  - Support Points Shape: {support_points.shape}\")\n",
    "    print(f\"  - Prior Weights Shape: {prior_weights.shape}\")\n",
    "\n",
    "for modality_idx, (denoise_func, ddenoise_func) in modality_denoisers.items():\n",
    "    print(f\"Modality {modality_idx} has a denoiser and derivative function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster Prior Information:\n",
      "\n",
      "Cluster 1:\n",
      "  - Support Points Shape: (50, 12)\n",
      "  - Prior Weights Shape: (50,)\n",
      "Cluster 0:\n",
      "  - Support Points Shape: (50, 12)\n",
      "  - Prior Weights Shape: (50,)\n",
      "\n",
      "Applying Modality-Specific Denoisers:\n",
      "\n",
      "Modality 0 (Cluster 1):\n",
      "  - Denoised Values Shape: (100, 4) (Expected: (100, 4))\n",
      "  - Denoiser Derivative Shape: (100, 4, 4)\n",
      "Modality 1 (Cluster 1):\n",
      "  - Denoised Values Shape: (100, 4) (Expected: (100, 4))\n",
      "  - Denoiser Derivative Shape: (100, 4, 4)\n",
      "Modality 2 (Cluster 0):\n",
      "  - Denoised Values Shape: (100, 3) (Expected: (100, 3))\n",
      "  - Denoiser Derivative Shape: (100, 3, 3)\n",
      "Modality 3 (Cluster 0):\n",
      "  - Denoised Values Shape: (100, 4) (Expected: (100, 4))\n",
      "  - Denoiser Derivative Shape: (100, 4, 4)\n",
      "Modality 4 (Cluster 1):\n",
      "  - Denoised Values Shape: (100, 4) (Expected: (100, 4))\n",
      "  - Denoiser Derivative Shape: (100, 4, 4)\n",
      "Modality 5 (Cluster 0):\n",
      "  - Denoised Values Shape: (100, 5) (Expected: (100, 5))\n",
      "  - Denoiser Derivative Shape: (100, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCluster Prior Information:\\n\")\n",
    "for cluster_idx in ceb.cluster_priors:\n",
    "    support_points, prior_weights = ceb.cluster_priors[cluster_idx]\n",
    "    \n",
    "    print(f\"Cluster {cluster_idx}:\")\n",
    "    print(f\"  - Support Points Shape: {support_points.shape}\")\n",
    "    print(f\"  - Prior Weights Shape: {prior_weights.shape}\")\n",
    "\n",
    "print(\"\\nApplying Modality-Specific Denoisers:\\n\")\n",
    "\n",
    "for modality_idx, (denoise_func, ddenoise_func) in ceb.modality_denoisers.items():\n",
    "    cluster_idx = cluster_labels[modality_idx]  # Find cluster of this modality\n",
    "    X_cluster = ceb.cluster_data[cluster_idx]  # Get aggregated cluster data\n",
    "    M_cluster = ceb.cluster_M[cluster_idx]  # Get aggregated M matrix\n",
    "    S_cluster = ceb.cluster_S[cluster_idx]  # Get aggregated S matrix\n",
    "\n",
    "    # Compute correct start and end column indices inside cluster-level aggregated data\n",
    "    modality_cols = [m.shape[1] for m, c in zip(ceb.data_matrices, ceb.cluster_labels) if c == cluster_idx]\n",
    "    start_col = sum(modality_cols[:modality_idx - sum(1 for c in ceb.cluster_labels[:modality_idx] if c != cluster_idx)])\n",
    "    end_col = start_col + ceb.data_matrices[modality_idx].shape[1]\n",
    "\n",
    "    # Apply denoiser\n",
    "    denoised_values = denoise_func(X_cluster, M_cluster, S_cluster)\n",
    "    denoiser_derivative = ddenoise_func(X_cluster, M_cluster, S_cluster)\n",
    "\n",
    "    # Extract only the relevant modality's part from the cluster-level denoised data\n",
    "    denoised_values = denoised_values\n",
    "    denoiser_derivative = denoiser_derivative\n",
    "\n",
    "    print(f\"Modality {modality_idx} (Cluster {cluster_idx}):\")\n",
    "    print(f\"  - Denoised Values Shape: {denoised_values.shape} (Expected: {data_matrices[modality_idx].shape})\")\n",
    "    print(f\"  - Denoiser Derivative Shape: {denoiser_derivative.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of all data matrices:\n",
      "Modality 0: (100, 4)\n",
      "Modality 1: (100, 4)\n",
      "Modality 2: (100, 3)\n",
      "Modality 3: (100, 4)\n",
      "Modality 4: (100, 4)\n",
      "Modality 5: (100, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDimensions of all data matrices:\")\n",
    "for idx, X in enumerate(data_matrices):\n",
    "    print(f\"Modality {idx}: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HierarchicalAMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
