{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.linalg import block_diag\n",
    "from abc import ABC, abstractmethod\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def _npmle_em_hd(f, Z, mu, covInv, em_iter, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Performs Nonparametric Maximum Likelihood Estimation (NPMLE) using EM.\n",
    "\n",
    "    - Ensures no division by zero in posterior probabilities.\n",
    "    - Ensures `pi` remains well-defined.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : ndarray (n_samples, d)\n",
    "        Observed data points.\n",
    "    Z : ndarray (n_support, d)\n",
    "        Support points.\n",
    "    mu : ndarray (d, d)\n",
    "        Mean transformation matrix.\n",
    "    covInv : ndarray (d, d)\n",
    "        Inverse covariance matrix.\n",
    "    em_iter : int\n",
    "        Number of EM iterations.\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small positive value to prevent division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : ndarray (n_support,)\n",
    "        Estimated prior probabilities.\n",
    "    \"\"\"\n",
    "    nsupp = Z.shape[0]\n",
    "    pi = np.full(nsupp, 1 / nsupp, dtype=np.float64)  # Initialize uniform prior\n",
    "\n",
    "    W = _get_W(f, Z, mu, covInv, eps=eps)\n",
    "\n",
    "    Wt = np.array(W.T, order='C')\n",
    "\n",
    "    for _ in range(em_iter):\n",
    "        denom = np.clip(W @ pi, eps, np.inf)  # Prevent division by zero\n",
    "        update_factor = np.mean(Wt / denom, axis=1)\n",
    "        update_factor = np.nan_to_num(update_factor, nan=1/nsupp, posinf=1/nsupp, neginf=1/nsupp)\n",
    "\n",
    "        pi *= update_factor\n",
    "        pi /= np.sum(pi)  # Normalize to sum to 1\n",
    "\n",
    "    return pi\n",
    "\n",
    "# consider another get W using broadcast\n",
    "# W[i,j] = f(x_i | z_j)\n",
    "def _get_W(f, z, mu, covInv, clip_max=50, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute likelihood matrix W[i, j] = P(X_i | Z_j), ensuring numerical stability.\n",
    "\n",
    "    - Ensures all exponentiation is within safe limits.\n",
    "    - Prevents division by zero.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : ndarray (n_samples, d)\n",
    "        Observed data points.\n",
    "    z : ndarray (n_support, d)\n",
    "        Support points.\n",
    "    mu : ndarray (d, d)\n",
    "        Mean matrix.\n",
    "    covInv : ndarray (d, d)\n",
    "        Inverse covariance matrix.\n",
    "    clip_max : float, optional (default=50)\n",
    "        Maximum absolute value for exponentiation to prevent numerical issues.\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small value to prevent division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W : ndarray (n_samples, n_support)\n",
    "        Likelihood matrix.\n",
    "    \"\"\"\n",
    "    f, z, mu, covInv = [arr.astype(np.float64) for arr in [f, z, mu, covInv]]\n",
    "\n",
    "    fsq = (np.einsum(\"ij,ij->i\", f @ covInv, f) / 2)[:, np.newaxis]  # Shape: (n_samples, 1)\n",
    "    mz = z @ mu.T  # Shape: (n_support, d)\n",
    "    zsq = (np.einsum(\"ij,ij->i\", mz @ covInv, mz) / 2)[:, np.newaxis]  # Shape: (n_support, 1)\n",
    "    \n",
    "    exponent = -fsq - zsq.T + (f @ covInv @ mz.T)  # Shape: (n_samples, n_support)\n",
    "\n",
    "    # Clip exponent to prevent overflow in exp\n",
    "    exponent = np.clip(exponent, -clip_max, clip_max)\n",
    "\n",
    "    # Compute likelihood matrix W and ensure no zero values\n",
    "    W = np.exp(exponent)\n",
    "    W = np.maximum(W, eps)  # Prevent exact zero values\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "# P[i,j] = P(Z_j | X_i)\n",
    "def _get_P(f, z, mu, covInv, pi, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute posterior probability matrix P[i, j] = P(Z_j | X_i).\n",
    "\n",
    "    - Ensures no division by zero.\n",
    "    - Prevents `NaN` values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : ndarray (n_samples, d)\n",
    "        Observed data points.\n",
    "    z : ndarray (n_support, d)\n",
    "        Support points.\n",
    "    mu : ndarray (d, d)\n",
    "        Mean transformation matrix.\n",
    "    covInv : ndarray (d, d)\n",
    "        Inverse covariance matrix.\n",
    "    pi : ndarray (n_support,)\n",
    "        Prior probabilities.\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small positive value to prevent division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    P : ndarray (n_samples, n_support)\n",
    "        Posterior probability matrix.\n",
    "    \"\"\"\n",
    "    W = _get_W(f, z, mu, covInv, eps=eps)\n",
    "    \n",
    "    denom = np.clip(W @ pi, eps, np.inf)  # Prevent division by zero\n",
    "    num = W * pi  # Element-wise multiplication\n",
    "\n",
    "    P = num / denom[:, np.newaxis]  # Normalize probabilities\n",
    "    P = np.nan_to_num(P, nan=1/len(pi))  # Replace NaNs with uniform distribution\n",
    "\n",
    "    return P\n",
    "\n",
    "def _get_P_from_W(W, pi, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute posterior probability matrix P[i, j] = P(Z_j | X_i) from likelihood matrix W.\n",
    "\n",
    "    - Ensures no division by zero.\n",
    "    - Prevents `NaN` values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : ndarray (n_samples, n_support)\n",
    "        Likelihood matrix.\n",
    "    pi : ndarray (n_support,)\n",
    "        Prior probabilities.\n",
    "    eps : float, optional (default=1e-8)\n",
    "        Small positive value to prevent division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    P : ndarray (n_samples, n_support)\n",
    "        Posterior probability matrix.\n",
    "    \"\"\"\n",
    "    denom = np.clip(W @ pi, eps, np.inf)  # Prevent division by zero\n",
    "    num = W * pi  # Element-wise multiplication\n",
    "\n",
    "    P = num / denom[:, np.newaxis]  \n",
    "    P = np.nan_to_num(P, nan=1/len(pi))  # Replace NaNs with uniform distribution\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "matrix_outer = lambda A, B: np.einsum(\"bi,bo->bio\", A, B)\n",
    "\n",
    "class _BaseEmpiricalBayes(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for Empirical Bayes estimation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    estimate_prior(f, mu, cov):\n",
    "        Abstract method to estimate the prior distribution.\n",
    "    denoise(f, mu, cov):\n",
    "        Abstract method to denoise posterior observations.\n",
    "    ddenoise(f, mu, cov):\n",
    "        Abstract method to compute the derivative of the denoising function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rank = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def estimate_prior(self, f, mu, cov):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def denoise(self, f, mu, cov):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def ddenoise(self, f, mu, cov):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NonparEB(_BaseEmpiricalBayes):\n",
    "    \"\"\"\n",
    "    NPMLE-based empirical Bayes (only supports EM optimizer).\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    estimate_prior(f, mu, cov):\n",
    "        Estimates the prior distribution using the EM algorithm.\n",
    "    denoise(f, mu, cov):\n",
    "        Computes the posterior mean estimates with NaN handling.\n",
    "    ddenoise(f, mu, cov):\n",
    "        Computes the derivative of the denoising function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_nsupp=2000, nsupp_ratio=1, em_iter=500):\n",
    "        super().__init__()\n",
    "        self.em_iter = em_iter\n",
    "        self.nsupp_ratio = nsupp_ratio\n",
    "        self.max_nsupp = max_nsupp\n",
    "        self.pi = None\n",
    "        self.Z = None\n",
    "\n",
    "    def _check_init(self, f, mu):\n",
    "        self.rank = mu.shape[1]\n",
    "        self.nsample = f.shape[0]\n",
    "        self.nsupp = min(int(self.nsupp_ratio * self.nsample), self.max_nsupp or float('inf'))\n",
    "        self.pi = np.full((self.nsupp,), 1 / self.nsupp)\n",
    "\n",
    "        # Compute support points (Z) with `pinv` for stability\n",
    "        if self.nsupp_ratio >= 1:\n",
    "            self.Z = f @ np.linalg.pinv(mu).T\n",
    "        else:\n",
    "            idx = np.random.choice(f.shape[0], self.nsupp, replace=False)\n",
    "            self.Z = f[idx, :] @ np.linalg.pinv(mu).T\n",
    "\n",
    "    def estimate_prior(self, f, mu, cov):\n",
    "        self._check_init(f, mu)\n",
    "        covInv = np.linalg.pinv(cov)  # Use pseudo-inverse for stability\n",
    "        self.pi = _npmle_em_hd(f, self.Z, mu, covInv, self.em_iter)\n",
    "        return self.Z, self.pi  # Return support points and probability weights\n",
    "\n",
    "    def denoise(self, f, mu, cov, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Compute the denoised posterior estimates with NaN handling.\n",
    "\n",
    "        - Ensures rows with NaN values are replaced by prior mean.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : ndarray (n, d)\n",
    "            Observed data points.\n",
    "        mu : ndarray (d, d)\n",
    "            Mean transformation matrix.\n",
    "        cov : ndarray (d, d)\n",
    "            Covariance matrix of the prior.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        denoised_values : ndarray (n, d)\n",
    "            Posterior mean estimates with NaN handling.\n",
    "        \"\"\"\n",
    "        covInv = np.linalg.pinv(cov)  # Use pseudo-inverse for numerical stability\n",
    "        P = _get_P(f, self.Z, mu, covInv, self.pi)\n",
    "\n",
    "        denoised_values = P @ self.Z  # Compute posterior mean\n",
    "\n",
    "        # Identify rows with NaN values and replace them with prior mean\n",
    "        nan_rows = np.isnan(denoised_values).any(axis=1)\n",
    "        if np.any(nan_rows):\n",
    "            warnings.warn(\"NaN detected in denoised output. Replacing affected rows with prior mean.\", RuntimeWarning)\n",
    "            denoised_values[nan_rows] = np.average(self.Z, axis=0, weights=self.pi)\n",
    "\n",
    "        return np.nan_to_num(denoised_values, nan=0.0)  # Ensure no NaNs remain\n",
    "\n",
    "    def ddenoise(self, f, mu, cov):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the denoising function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : ndarray (n, d)\n",
    "            Observed data points.\n",
    "        mu : ndarray (d, d)\n",
    "            Mean transformation matrix.\n",
    "        cov : ndarray (d, d)\n",
    "            Covariance matrix of the prior.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative : ndarray\n",
    "            The derivative of the denoising function at the posterior observations.\n",
    "        \"\"\"\n",
    "        covInv = np.linalg.pinv(cov)\n",
    "        P = _get_P(f, self.Z, mu, covInv, self.pi)\n",
    "        ZouterMZ = np.einsum(\"ijk, kl -> ijl\", matrix_outer(self.Z, self.Z @ mu.T), covInv)\n",
    "        E1 = np.einsum(\"ij, jkl -> ikl\", P, ZouterMZ)\n",
    "        E2a = P @ self.Z\n",
    "        E2 = np.einsum(\"ijk, kl -> ijl\", matrix_outer(E2a, E2a @ mu.T), covInv)\n",
    "\n",
    "        return E1 - E2\n",
    "\n",
    "class NonparBayes(NonparEB):\n",
    "    \"\"\"\n",
    "    Nonparametric Bayes with a Known Prior.\n",
    "\n",
    "    This class extends `NonparEB` but does not estimate a prior from data. \n",
    "    Instead, it takes a known prior (locations and weights) as input.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Z : ndarray\n",
    "        The known prior locations (support points).\n",
    "    pi : ndarray\n",
    "        The weights associated with the prior locations.\n",
    "    rank : int\n",
    "        Dimensionality of the prior distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, truePriorLoc, truePriorWeight=None):\n",
    "        \"\"\"\n",
    "        Initialize Nonparametric Bayes model with a known prior.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        truePriorLoc : ndarray of shape (n, k)\n",
    "            The known prior locations, where `n` is the number of support points \n",
    "            and `k` is the dimensionality.\n",
    "        truePriorWeight : ndarray of shape (n,), optional\n",
    "            The probability weights associated with `truePriorLoc`. If not provided, \n",
    "            a uniform distribution over `n` points is assumed.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the provided prior locations and weights do not match dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure `truePriorLoc` is a 2D array\n",
    "        if truePriorLoc.ndim != 2:\n",
    "            raise ValueError(\"truePriorLoc must be a 2D array of shape (n, k)\")\n",
    "\n",
    "        n, k = truePriorLoc.shape\n",
    "\n",
    "        self.Z = truePriorLoc.astype(np.float64)  # Ensure numerical stability\n",
    "        self.rank = k  # Dimensionality of the prior distribution\n",
    "\n",
    "        # Store prior weights (uniform if not provided)\n",
    "        if truePriorWeight is None:\n",
    "            self.pi = np.full((n,), 1 / n, dtype=np.float64)\n",
    "        else:\n",
    "            if truePriorWeight.ndim != 1:\n",
    "                raise ValueError(\"truePriorWeight must be a 1D array of shape (n,)\")\n",
    "            if truePriorWeight.shape[0] != n:\n",
    "                raise ValueError(f\"truePriorWeight must match truePriorLoc in size ({n},)\")\n",
    "\n",
    "            self.pi = np.array(truePriorWeight, dtype=np.float64)\n",
    "\n",
    "    def estimate_prior(self, f, mu, cov):\n",
    "        \"\"\"\n",
    "        No prior estimation is needed since the prior is already given.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ClusterEmpiricalBayes:\n",
    "    \"\"\"\n",
    "    Handles clustering of modalities, aggregation of data, and estimation of empirical Bayes priors.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cluster_data : dict\n",
    "        Maps each cluster to its concatenated data matrix.\n",
    "    cluster_M : dict\n",
    "        Maps each cluster to its block-diagonal M matrix.\n",
    "    cluster_S : dict\n",
    "        Maps each cluster to its block-diagonal S matrix.\n",
    "    cluster_priors : dict\n",
    "        Maps each cluster to (support_points, prior_weights).\n",
    "    modality_denoisers : dict\n",
    "        Maps each modality index to a function that extracts its denoised values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_matrices, M_matrices, S_matrices, cluster_labels):\n",
    "        \"\"\"\n",
    "        Initialize the ClusterEmpiricalBayes class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_matrices : list of ndarrays\n",
    "            List of m data matrices X_k of shape (n, r_k).\n",
    "        M_matrices : list of ndarrays\n",
    "            List of m transformation matrices M_k of shape (r_k, p_k).\n",
    "        S_matrices : list of ndarrays\n",
    "            List of m noise matrices S_k of shape (r_k, r_k).\n",
    "        cluster_labels : list or ndarray\n",
    "            Cluster labels of length m, indicating the cluster index for each modality.\n",
    "        \"\"\"\n",
    "        if not (len(data_matrices) == len(M_matrices) == len(S_matrices) == len(cluster_labels)):\n",
    "            raise ValueError(\"Mismatch in number of modalities among data_matrices, M_matrices, S_matrices, and cluster_labels.\")\n",
    "\n",
    "        self.data_matrices = data_matrices  # Store raw data per modality\n",
    "        self.cluster_labels = cluster_labels  # Store cluster assignments for modalities\n",
    "\n",
    "        # Aggregate cluster data\n",
    "        self.cluster_data, self.cluster_M, self.cluster_S = self.aggregate_cluster_data(\n",
    "            data_matrices, M_matrices, S_matrices, cluster_labels\n",
    "        )\n",
    "\n",
    "        # Dictionary to store cluster priors\n",
    "        self.cluster_priors = {}\n",
    "\n",
    "        # Dictionary to store denoising functions for each modality\n",
    "        self.modality_denoisers = {}\n",
    "\n",
    "    def aggregate_cluster_data(self, data_matrices, M_matrices, S_matrices, cluster_labels):\n",
    "        \"\"\"\n",
    "        Aggregates data, M, and S matrices based on cluster labels and constructs block-diagonal M and S.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_data, cluster_M, cluster_S : dict\n",
    "            Dictionaries mapping each cluster index to its aggregated data, block-diagonal M, and block-diagonal S.\n",
    "        \"\"\"\n",
    "        cluster_data = defaultdict(list)\n",
    "        cluster_M = defaultdict(list)\n",
    "        cluster_S = defaultdict(list)\n",
    "\n",
    "        for k, cluster in enumerate(cluster_labels):\n",
    "            cluster_data[cluster].append(data_matrices[k].astype(np.float64))  # Ensure numerical stability\n",
    "            cluster_M[cluster].append(M_matrices[k].astype(np.float64))\n",
    "            cluster_S[cluster].append(S_matrices[k].astype(np.float64))\n",
    "\n",
    "        for cluster in cluster_data:\n",
    "            sample_sizes = [X.shape[0] for X in cluster_data[cluster]]\n",
    "            if len(set(sample_sizes)) > 1:\n",
    "                raise ValueError(f\"Mismatch in sample sizes for cluster {cluster}: {sample_sizes}\")\n",
    "\n",
    "            cluster_data[cluster] = np.concatenate(cluster_data[cluster], axis=1)\n",
    "            cluster_M[cluster] = block_diag(*cluster_M[cluster])\n",
    "            cluster_S[cluster] = block_diag(*cluster_S[cluster])\n",
    "\n",
    "        return cluster_data, cluster_M, cluster_S\n",
    "\n",
    "    def estimate_cluster_priors(self, em_iter=500, nsupp_ratio=0.5, max_nsupp=100):\n",
    "        \"\"\"\n",
    "        Estimates priors (per-cluster) and denoisers (per-modality) using Nonparametric Empirical Bayes.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cluster_priors : dict\n",
    "            Dictionary mapping each cluster to (support_points, prior_weights).\n",
    "        modality_denoisers : dict\n",
    "            Dictionary mapping each modality index to a function that extracts its denoised values.\n",
    "        \"\"\"\n",
    "        cluster_denoisers = {}\n",
    "\n",
    "        # Estimate priors at the cluster level\n",
    "        for cluster in self.cluster_data:\n",
    "            X_cluster = self.cluster_data[cluster]\n",
    "            M_cluster = self.cluster_M[cluster]\n",
    "            S_cluster = self.cluster_S[cluster]\n",
    "\n",
    "            if X_cluster.shape[1] != M_cluster.shape[0]:\n",
    "                raise ValueError(f\"Mismatch in dimensions for cluster {cluster}: X ({X_cluster.shape}) and M ({M_cluster.shape})\")\n",
    "            if S_cluster.shape[0] != S_cluster.shape[1]:\n",
    "                raise ValueError(f\"Noise matrix S for cluster {cluster} is not square: {S_cluster.shape}\")\n",
    "            if S_cluster.shape[0] != M_cluster.shape[0]:\n",
    "                raise ValueError(f\"Mismatch in S ({S_cluster.shape}) and M ({M_cluster.shape}) for cluster {cluster}\")\n",
    "\n",
    "            # Estimate prior using empirical Bayes\n",
    "            nonpar_eb = NonparEB(em_iter=em_iter, nsupp_ratio=nsupp_ratio, max_nsupp=max_nsupp)\n",
    "            support_points, prior_weights = nonpar_eb.estimate_prior(X_cluster, M_cluster, S_cluster)\n",
    "\n",
    "            # Store prior per cluster\n",
    "            self.cluster_priors[cluster] = (support_points, prior_weights)\n",
    "            cluster_denoisers[cluster] = nonpar_eb  # Store corresponding denoiser object\n",
    "\n",
    "        # Define denoisers per modality\n",
    "        for modality_idx, cluster in enumerate(self.cluster_labels):\n",
    "            start_col = sum(\n",
    "                self.data_matrices[i].shape[1] for i in range(modality_idx) if self.cluster_labels[i] == cluster\n",
    "            )\n",
    "            end_col = start_col + self.data_matrices[modality_idx].shape[1]\n",
    "\n",
    "            nonpar_eb = cluster_denoisers[cluster]  # Use cluster-specific prior\n",
    "\n",
    "            def create_denoise_func(nonpar_eb, start_col, end_col):\n",
    "                def denoise_func(f, mu, cov):\n",
    "                    denoised_cluster = nonpar_eb.denoise(f, mu, cov)\n",
    "                    return denoised_cluster[:, start_col:end_col]\n",
    "                return denoise_func\n",
    "\n",
    "            def create_ddenoise_func(nonpar_eb, start_col, end_col):\n",
    "                def ddenoise_func(f, mu, cov):\n",
    "                    ddenoised_cluster = nonpar_eb.ddenoise(f, mu, cov)\n",
    "                    return ddenoised_cluster[:, start_col:end_col, start_col:end_col]\n",
    "                return ddenoise_func\n",
    "            \n",
    "            # Store the denoisers with explicit names\n",
    "            self.modality_denoisers[modality_idx] = {\n",
    "                \"denoise\": create_denoise_func(nonpar_eb, start_col, end_col),\n",
    "                \"ddenoise\": create_ddenoise_func(nonpar_eb, start_col, end_col),\n",
    "            }\n",
    "\n",
    "        return self.cluster_priors, self.modality_denoisers\n",
    "\n",
    "def generate_synthetic_data(num_modalities=6, num_clusters=3, n=100, r_range=(3, 7), noise_scale=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data matrices X_k = M_k U_k + S_k^{1/2} Z_k with cluster-correlated latent factors.\n",
    "\n",
    "    - Ensures numerical stability (`np.float64`).\n",
    "    - Prevents zero or negative values in noise covariance matrices.\n",
    "    - Avoids `NaN` values in generated data.\n",
    "    - Ensures each cluster has at least one modality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_modalities : int\n",
    "        Number of different modalities.\n",
    "    num_clusters : int\n",
    "        Number of clusters.\n",
    "    n : int\n",
    "        Number of samples (same across all modalities).\n",
    "    r_range : tuple\n",
    "        Range of values for r_k (dimensionality of each modality).\n",
    "    noise_scale : float\n",
    "        Standard deviation of noise components.\n",
    "    seed : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_matrices : list of ndarrays\n",
    "        Generated X_k data matrices of different dimensions.\n",
    "    M_matrices : list of ndarrays\n",
    "        Transformation matrices M_k of different sizes.\n",
    "    S_matrices : list of ndarrays\n",
    "        Diagonal noise matrices S_k of different sizes.\n",
    "    cluster_labels : ndarray\n",
    "        Cluster assignments for each modality.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)  # Set seed for reproducibility\n",
    "\n",
    "    # Assign each modality to a cluster\n",
    "    cluster_labels = np.random.randint(0, num_clusters, size=num_modalities).astype(np.int64)\n",
    "\n",
    "    # Determine r_k for each modality, ensuring positive dimensions\n",
    "    modality_dims = np.random.randint(r_range[0], r_range[1] + 1, size=num_modalities)\n",
    "\n",
    "    # Ensure each cluster has at least one modality\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "    # Compute the minimum r_c for each cluster\n",
    "    cluster_min_dims = {c: min(modality_dims[cluster_labels == c]) for c in unique_clusters}\n",
    "\n",
    "    # Generate shared cluster-wise latent variables U_c\n",
    "    cluster_latents = {c: np.random.randn(n, cluster_min_dims[c]).astype(np.float64) for c in unique_clusters}\n",
    "\n",
    "    # Initialize lists for data matrices, transformation matrices, and noise matrices\n",
    "    data_matrices = []\n",
    "    M_matrices = []\n",
    "    S_matrices = []\n",
    "\n",
    "    for k in range(num_modalities):\n",
    "        r_k = modality_dims[k]  # Dimension of modality k\n",
    "        cluster_idx = cluster_labels[k]  # Assigned cluster\n",
    "        r_c = cluster_min_dims[cluster_idx]  # Minimum r_c in the cluster\n",
    "\n",
    "        # Generate `U_k`, ensuring numerical stability\n",
    "        U_k = np.hstack([\n",
    "            cluster_latents[cluster_idx],  # First r_c columns from U_c\n",
    "            np.random.randn(n, r_k - r_c).astype(np.float64) if r_k > r_c else np.empty((n, 0))\n",
    "        ])\n",
    "\n",
    "        # Generate transformation matrix M_k (r_k × r_k), ensuring no zero diagonal elements\n",
    "        M_k = np.diag(np.random.uniform(0.5, 1.5, size=r_k).astype(np.float64))\n",
    "\n",
    "        # Generate noise matrix S_k (diagonal, r_k × r_k) with minimum values\n",
    "        S_k_diag = np.clip(np.random.uniform(0.05, noise_scale, size=r_k), 1e-4, np.inf)\n",
    "        S_k = np.diag(S_k_diag).astype(np.float64)\n",
    "\n",
    "        # Generate noise Z_k, ensuring numerical stability\n",
    "        Z_k = np.random.randn(n, r_k).astype(np.float64)\n",
    "\n",
    "        # Compute X_k = M_k U_k + S_k^{1/2} Z_k\n",
    "        X_k = (U_k @ M_k.T) + (Z_k @ np.sqrt(S_k))\n",
    "\n",
    "        # Store results\n",
    "        data_matrices.append(X_k)\n",
    "        M_matrices.append(M_k)\n",
    "        S_matrices.append(S_k)\n",
    "\n",
    "    return data_matrices, M_matrices, S_matrices, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data with cluster-correlated modalities\n",
    "data_matrices, M_matrices, S_matrices, cluster_labels = generate_synthetic_data(num_modalities=6, num_clusters=3, seed = 10)\n",
    "\n",
    "# Print cluster assignments\n",
    "print(\"Cluster Labels:\", cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceb = ClusterEmpiricalBayes(data_matrices, M_matrices, S_matrices, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_priors, modality_denoisers = ceb.estimate_cluster_priors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceb.modality_denoisers[1]['denoise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 1: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 2: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 3: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 4: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>),\n",
       " 5: (<function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_denoise_func.<locals>.denoise_func(f, mu, cov)>,\n",
       "  <function __main__.ClusterEmpiricalBayes.estimate_cluster_priors.<locals>.create_ddenoise_func.<locals>.ddenoise_func(f, mu, cov)>)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modality_denoisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      "  - Support Points Shape: (50, 10)\n",
      "  - Prior Weights Shape: (50,)\n",
      "Cluster 0:\n",
      "  - Support Points Shape: (50, 13)\n",
      "  - Prior Weights Shape: (50,)\n",
      "Modality 0 has a denoiser and derivative function.\n",
      "Modality 1 has a denoiser and derivative function.\n",
      "Modality 2 has a denoiser and derivative function.\n",
      "Modality 3 has a denoiser and derivative function.\n",
      "Modality 4 has a denoiser and derivative function.\n",
      "Modality 5 has a denoiser and derivative function.\n"
     ]
    }
   ],
   "source": [
    "for cluster_idx, (support_points, prior_weights) in cluster_priors.items():\n",
    "    print(f\"Cluster {cluster_idx}:\")\n",
    "    print(f\"  - Support Points Shape: {support_points.shape}\")\n",
    "    print(f\"  - Prior Weights Shape: {prior_weights.shape}\")\n",
    "\n",
    "for modality_idx, (denoise_func, ddenoise_func) in modality_denoisers.items():\n",
    "    print(f\"Modality {modality_idx} has a denoiser and derivative function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster Prior Information:\n",
      "\n",
      "Cluster 1:\n",
      "  - Support Points Shape: (50, 10)\n",
      "  - Prior Weights Shape: (50,)\n",
      "Cluster 0:\n",
      "  - Support Points Shape: (50, 13)\n",
      "  - Prior Weights Shape: (50,)\n",
      "\n",
      "Applying Modality-Specific Denoisers:\n",
      "\n",
      "Modality 0 (Cluster 1):\n",
      "  - Denoised Values Shape: (100, 4) (Expected: (100, 4))\n",
      "  - Denoiser Derivative Shape: (100, 4, 4)\n",
      "Modality 1 (Cluster 1):\n",
      "  - Denoised Values Shape: (100, 3) (Expected: (100, 3))\n",
      "  - Denoiser Derivative Shape: (100, 3, 3)\n",
      "Modality 2 (Cluster 0):\n",
      "  - Denoised Values Shape: (100, 4) (Expected: (100, 4))\n",
      "  - Denoiser Derivative Shape: (100, 4, 4)\n",
      "Modality 3 (Cluster 0):\n",
      "  - Denoised Values Shape: (100, 5) (Expected: (100, 5))\n",
      "  - Denoiser Derivative Shape: (100, 5, 5)\n",
      "Modality 4 (Cluster 1):\n",
      "  - Denoised Values Shape: (100, 3) (Expected: (100, 3))\n",
      "  - Denoiser Derivative Shape: (100, 3, 3)\n",
      "Modality 5 (Cluster 0):\n",
      "  - Denoised Values Shape: (100, 4) (Expected: (100, 4))\n",
      "  - Denoiser Derivative Shape: (100, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCluster Prior Information:\\n\")\n",
    "for cluster_idx in ceb.cluster_priors:\n",
    "    support_points, prior_weights = ceb.cluster_priors[cluster_idx]\n",
    "    \n",
    "    print(f\"Cluster {cluster_idx}:\")\n",
    "    print(f\"  - Support Points Shape: {support_points.shape}\")\n",
    "    print(f\"  - Prior Weights Shape: {prior_weights.shape}\")\n",
    "\n",
    "print(\"\\nApplying Modality-Specific Denoisers:\\n\")\n",
    "\n",
    "for modality_idx, (denoise_func, ddenoise_func) in ceb.modality_denoisers.items():\n",
    "    cluster_idx = cluster_labels[modality_idx]  # Find cluster of this modality\n",
    "    X_cluster = ceb.cluster_data[cluster_idx]  # Get aggregated cluster data\n",
    "    M_cluster = ceb.cluster_M[cluster_idx]  # Get aggregated M matrix\n",
    "    S_cluster = ceb.cluster_S[cluster_idx]  # Get aggregated S matrix\n",
    "\n",
    "    # Compute correct start and end column indices inside cluster-level aggregated data\n",
    "    modality_cols = [m.shape[1] for m, c in zip(ceb.data_matrices, ceb.cluster_labels) if c == cluster_idx]\n",
    "    start_col = sum(modality_cols[:modality_idx - sum(1 for c in ceb.cluster_labels[:modality_idx] if c != cluster_idx)])\n",
    "    end_col = start_col + ceb.data_matrices[modality_idx].shape[1]\n",
    "\n",
    "    # Apply denoiser\n",
    "    denoised_values = denoise_func(X_cluster, M_cluster, S_cluster)\n",
    "    denoiser_derivative = ddenoise_func(X_cluster, M_cluster, S_cluster)\n",
    "\n",
    "    # Extract only the relevant modality's part from the cluster-level denoised data\n",
    "    denoised_values = denoised_values\n",
    "    denoiser_derivative = denoiser_derivative\n",
    "\n",
    "    print(f\"Modality {modality_idx} (Cluster {cluster_idx}):\")\n",
    "    print(f\"  - Denoised Values Shape: {denoised_values.shape} (Expected: {data_matrices[modality_idx].shape})\")\n",
    "    print(f\"  - Denoiser Derivative Shape: {denoiser_derivative.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.32532211, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.93423801, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.65683869, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.9773599 ]]),\n",
       " array([[0.95958691, 0.        , 0.        ],\n",
       "        [0.        , 0.81734248, 0.        ],\n",
       "        [0.        , 0.        , 0.95367312]]),\n",
       " array([[0.54942317, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.54736408, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.60039336, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.648547  ]]),\n",
       " array([[0.66563768, 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.59104224, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 1.02194354, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 1.34183127, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.8422337 ]]),\n",
       " array([[0.93055226, 0.        , 0.        ],\n",
       "        [0.        , 1.00351052, 0.        ],\n",
       "        [0.        , 0.        , 0.77241906]]),\n",
       " array([[0.83352205, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.10111052, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 1.21736229, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.97506229]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of all data matrices:\n",
      "Modality 0: (100, 4)\n",
      "Modality 1: (100, 3)\n",
      "Modality 2: (100, 4)\n",
      "Modality 3: (100, 5)\n",
      "Modality 4: (100, 3)\n",
      "Modality 5: (100, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDimensions of all data matrices:\")\n",
    "for idx, X in enumerate(data_matrices):\n",
    "    print(f\"Modality {idx}: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical_amp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
